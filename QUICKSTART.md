# ğŸš€ Guide de DÃ©marrage Rapide

## âš¡ Installation en 5 Minutes

### 1ï¸âƒ£ PrÃ©requis
```bash
# VÃ©rifier Python
python3 --version  # Doit Ãªtre 3.8+

# VÃ©rifier Docker
docker --version
docker-compose --version

# VÃ©rifier Java 17
java -version  # Doit Ãªtre 17.x
```

### 2ï¸âƒ£ Cloner et Installer
```bash
# Cloner le projet
git clone <votre-repo>
cd opensky-flight-tracker

# CrÃ©er l'environnement virtuel
python3 -m venv venv
source venv/bin/activate

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### 3ï¸âƒ£ DÃ©marrer Kafka
```bash
# Lancer Kafka avec Docker
docker-compose up -d

# VÃ©rifier que Kafka est actif
docker ps
# Vous devez voir : zookeeper, kafka, kafka-ui
```

### 4ï¸âƒ£ Lancer l'Application
```bash
# Option A : Script automatique (RECOMMANDÃ‰)
./start.sh

# Option B : Lancement manuel (3 terminaux)

# Terminal 1 - Producer
python kafka_producer.py

# Terminal 2 - Spark Consumer
export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
python spark_consumer.py

# Terminal 3 - Dashboard
streamlit run dashboard.py
```

### 5ï¸âƒ£ AccÃ©der au Dashboard
```
ğŸŒ Dashboard Streamlit : http://localhost:8501
ğŸ” Kafka UI : http://localhost:8080
```

---

## ğŸ› DÃ©pannage Express

### âŒ Kafka ne dÃ©marre pas
```bash
docker-compose down -v
docker-compose up -d
```

### âŒ Erreur Java avec Spark
```bash
# Installer Java 17
sudo apt install openjdk-17-jdk

# DÃ©finir JAVA_HOME
export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
```

### âŒ Aucun vol dÃ©tectÃ©
Zone Ouagadougou peut avoir peu de trafic. Pour tester, modifiez dans `kafka_producer.py` :
```python
# Changer les coordonnÃ©es pour Paris (beaucoup de vols)
OUAGA_LAT = 48.8566  # Paris
OUAGA_LON = 2.3522
RADIUS = 50
```

### âŒ Dashboard vide
1. Attendre 2-3 minutes que Spark agrÃ¨ge les donnÃ©es
2. VÃ©rifier que `spark_consumer.py` tourne
3. VÃ©rifier les logs : `tail -f /tmp/spark.log`

---

## ğŸ“Š Architecture SimplifiÃ©e

```
OpenSky API (toutes les 30s)
      â†“
kafka_producer.py (collecte et envoie vers Kafka)
      â†“
Kafka Topic: flights-data
      â†“
spark_consumer.py (traite et agrÃ¨ge en temps rÃ©el)
      â†“
Tables Spark en mÃ©moire
      â†“
dashboard.py (affiche avec Streamlit)
```

---

## ğŸ¯ Commandes Utiles

```bash
# Voir les logs du producer
tail -f /tmp/producer.log

# Voir les logs Spark
tail -f /tmp/spark.log

# Voir les logs Streamlit
tail -f /tmp/streamlit.log

# ArrÃªter tous les processus
pkill -f "kafka_producer|spark_consumer|streamlit"

# Nettoyer Kafka
docker-compose down -v

# VÃ©rifier les topics Kafka
docker exec -it kafka kafka-topics --list --bootstrap-server localhost:9092

# Lire les messages Kafka
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic flights-data \
  --from-beginning \
  --max-messages 5
```

---

## âœ… Checklist de VÃ©rification

- [ ] Python 3.8+ installÃ©
- [ ] Java 17 installÃ©
- [ ] Docker et Docker Compose installÃ©s
- [ ] Environnement virtuel crÃ©Ã©
- [ ] DÃ©pendances installÃ©es (`pip install -r requirements.txt`)
- [ ] Kafka dÃ©marrÃ© (`docker ps` montre 3 conteneurs)
- [ ] Producer lancÃ© (logs dans `/tmp/producer.log`)
- [ ] Spark Consumer lancÃ© (logs dans `/tmp/spark.log`)
- [ ] Dashboard accessible sur http://localhost:8501

---

## ğŸ“ Variables Ã  Personnaliser

### kafka_producer.py
```python
OUAGA_LAT = 12.3532  # Latitude de votre zone
OUAGA_LON = -1.5124  # Longitude de votre zone
RADIUS = 100         # Rayon en km
```

Ligne 163 :
```python
producer.run(interval=30)  # Intervalle entre requÃªtes API
```

### spark_consumer.py
Ligne 60 :
```python
topic='flights-data'  # Nom du topic Kafka
```

Ligne 90 :
```python
window("processing_time", "2 minutes", "1 minute")  # FenÃªtre d'agrÃ©gation
```

### dashboard.py
Ligne 211 :
```python
refresh_rate = st.sidebar.slider("...", 5, 60, 10)  # Taux de rafraÃ®chissement
```

---

## ğŸ“ Pour Votre Rapport

**Technologies utilisÃ©es :**
- Apache Kafka 7.5.0 (streaming)
- Apache Spark 3.5.0 (traitement)
- PySpark (API Python)
- Streamlit 1.29.0 (visualisation)
- OpenSky Network API (donnÃ©es)
- Docker Compose (infrastructure)

**Concepts dÃ©montrÃ©s :**
- Streaming temps rÃ©el
- Architecture microservices
- ETL (Extract, Transform, Load)
- AgrÃ©gations par fenÃªtres temporelles
- Visualisation interactive
- Conteneurisation

---

**ğŸ‰ Bon courage pour votre projet !**
