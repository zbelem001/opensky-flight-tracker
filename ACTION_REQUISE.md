# ğŸ”„ Action requise : Mise Ã  jour des images Docker Hub

## âš ï¸ ProblÃ¨me identifiÃ©

La fonction de **nettoyage automatique** (`cleanup_old_data()`) qui Ã©vite les conflits de cache Spark n'est **pas prÃ©sente** dans les images Docker Hub actuelles.

Cette fonction est importante car elle :
- âœ… Nettoie automatiquement les anciens fichiers Parquet au dÃ©marrage
- âœ… Ã‰vite les conflits de checkpoint Spark
- âœ… Garantit que le dashboard affiche des donnÃ©es fraÃ®ches

## ğŸ“‹ Ce qui doit Ãªtre fait

### Option 1 : Mise Ã  jour rapide (RecommandÃ©) âš¡

Utilisez le script automatique :

```bash
./update-and-publish.sh
```

Ce script va :
1. Committer les modifications
2. Pusher vers GitHub
3. Rebuilder les images Docker
4. Les publier sur Docker Hub

**Temps estimÃ©** : 10-15 minutes

### Option 2 : Mise Ã  jour manuelle ğŸ› ï¸

Si vous prÃ©fÃ©rez faire Ã©tape par Ã©tape :

#### 1. Committer les modifications
```bash
git add .
git commit -m "feat: Add automatic cleanup on Spark startup"
git push origin main
```

#### 2. Rebuilder les images
```bash
docker-compose build producer
docker-compose build spark-consumer
docker-compose build dashboard
```

#### 3. Tagger les images
```bash
docker tag opensky-flight-tracker_producer zbelem001/opensky-producer:latest
docker tag opensky-flight-tracker_spark-consumer zbelem001/opensky-spark:latest
docker tag opensky-flight-tracker_dashboard zbelem001/opensky-dashboard:latest
```

#### 4. Pusher sur Docker Hub
```bash
# Se connecter si nÃ©cessaire
docker login

# Pusher les images
docker push zbelem001/opensky-producer:latest
docker push zbelem001/opensky-spark:latest
docker push zbelem001/opensky-dashboard:latest
```

## ğŸ¯ Pour votre professeur

Une fois les images mises Ã  jour, votre professeur pourra :

```bash
# TÃ©lÃ©charger les derniÃ¨res versions
docker-compose -f docker-compose.hub.yml pull

# Lancer le projet
docker-compose -f docker-compose.hub.yml up -d
```

Le nettoyage automatique sera alors intÃ©grÃ© et le dashboard fonctionnera immÃ©diatement sans problÃ¨me de cache !

## ğŸ“ Modifications apportÃ©es

### Dans `spark_consumer.py`

```python
def cleanup_old_data(self):
    """Nettoie les anciens fichiers Parquet et checkpoint au dÃ©marrage"""
    data_path = os.getenv('FLIGHTS_DATA_PATH', '/tmp/flights_data')
    checkpoint_path = os.getenv('CHECKPOINT_PATH', '/data/checkpoint')
    
    for path in [data_path, checkpoint_path]:
        if os.path.exists(path):
            try:
                logger.info(f"ğŸ§¹ Nettoyage de {path}...")
                shutil.rmtree(path)
                logger.info(f"âœ… {path} nettoyÃ© avec succÃ¨s")
            except Exception as e:
                logger.warning(f"âš ï¸  Impossible de nettoyer {path}: {e}")
    
    # RecrÃ©er les rÃ©pertoires
    os.makedirs(data_path, exist_ok=True)
    os.makedirs(checkpoint_path, exist_ok=True)
```

Cette fonction est appelÃ©e automatiquement dans `__init__()`.

## â° Timing

Si vous ne pouvez pas mettre Ã  jour maintenant, votre professeur peut utiliser le workaround :

```bash
# Au lieu de juste up -d
docker-compose -f docker-compose.hub.yml down -v  # Nettoie les volumes
docker-compose -f docker-compose.hub.yml up -d    # RedÃ©marre proprement
```

Ou utiliser le script fourni :
```bash
./start-clean.sh
```

Mais il est **fortement recommandÃ©** de mettre Ã  jour les images pour que tout fonctionne automatiquement ! âœ¨

## ğŸš€ Status de mise Ã  jour

- [ ] Modifications committÃ©es sur GitHub
- [ ] Images Docker reconstruites
- [ ] Images publiÃ©es sur Docker Hub
- [ ] Guides mis Ã  jour (GUIDE_PROFESSEUR_SIMPLE.md, DEMARRAGE_RAPIDE.md)
- [ ] TestÃ© localement avec `docker-compose.hub.yml`

---

**Date** : 11 novembre 2025  
**Action** : Mise Ã  jour nÃ©cessaire avant remise du projet
